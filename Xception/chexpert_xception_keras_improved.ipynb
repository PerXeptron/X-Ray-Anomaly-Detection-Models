{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_xception.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zruJhT8gWxPi",
        "colab_type": "code",
        "outputId": "30c25215-f0d5-4a12-cab8-fcf35728583f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mehoxo4bXd9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -uq '/content/gdrive/My Drive/chexpertdataset.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dNr5OxL_P0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL5FP66LCekh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv('CheXpert-v1.0-small/train.csv')\n",
        "df_val = pd.read_csv('CheXpert-v1.0-small/valid.csv')\n",
        "# We shall make the train_test split later using ImageDataGenerator's parameter\n",
        "df = pd.concat([df_train, df_val])\n",
        "df = df.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7__0Zm1HAV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Focusing only on 5 classes:\n",
        "df = df[[\n",
        "  'Path', \n",
        "  'Atelectasis',\n",
        "  'Cardiomegaly',\n",
        "  'Consolidation',\n",
        "  'Edema',\n",
        "  'Pleural Effusion'\n",
        "]]\n",
        "\n",
        "# Handling the NaN values\n",
        "df = df.fillna(0)\n",
        "\n",
        "# Handling the uncertain values\n",
        "## Different policy for each feature:\n",
        "u_ones = ['Atelectasis', 'Edema']\n",
        "u_zeros = ['Cardiomegaly', 'Consolidation', 'Pleural Effusion']\n",
        "df[u_ones]  = df[u_ones].replace(-1, 1)\n",
        "df[u_zeros] = df[u_zeros].replace(-1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meSekdHZtPA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df = df.sample(frac=1)\n",
        "#df = df[:10000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rewlhwdGd1Uv",
        "colab_type": "text"
      },
      "source": [
        "# The Data Generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsWuowYLjCwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining constants:\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 224\n",
        "CLASSES = [ \n",
        "  'Atelectasis',\n",
        "  'Cardiomegaly',\n",
        "  'Consolidation',\n",
        "  'Edema',\n",
        "  'Pleural Effusion'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLeU2fkvIAL3",
        "colab_type": "text"
      },
      "source": [
        "Generating a sample for datagen fitting for featurewise normalization and featurewise centering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbZbXM3C7xHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants\n",
        "FRAC = 0.003 # Fraction of total data to be taken as sample\n",
        "SHAPE = (320, 390, 3) # Common shape for featurewise centering & normalization\n",
        "\n",
        "sample_paths = df['Path'].sample(frac=FRAC).to_numpy()\n",
        "X_temp = np.array([np.array(cv2.imread(path, 1), dtype=float) for path in sample_paths])\n",
        "X_sample = np.array([x for x in X_temp if x.shape == SHAPE])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwfAcCeHZJwN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f56b608-dfc7-478d-9515-5c0209ebf78e"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator as IDG\n",
        "\n",
        "datagen = IDG(\n",
        "    rescale=1./255, \n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=0.1,\n",
        "    zoom_range = 0.1,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split = 0.1,\n",
        "    fill_mode = 'nearest'\n",
        ")\n",
        "\n",
        "datagen.fit(X_sample)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HRgCZYRlq54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gen():\n",
        "  train_gen = datagen.flow_from_dataframe(\n",
        "      dataframe = df,\n",
        "      x_col = 'Path',\n",
        "      y_col = CLASSES, #'classes',\n",
        "      class_mode='raw',\n",
        "      #validate_filenames = False,\n",
        "      seed=42,\n",
        "      shuffle=True,\n",
        "      target_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
        "      batch_size=BATCH_SIZE, \n",
        "      subset = 'training'\n",
        "  )\n",
        "\n",
        "  val_gen = datagen.flow_from_dataframe(\n",
        "      dataframe = df,\n",
        "      x_col = 'Path',\n",
        "      y_col = CLASSES, #'classes',\n",
        "      class_mode='raw',\n",
        "      #validate_filenames = False,\n",
        "      seed=42,\n",
        "      shuffle=True,\n",
        "      target_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
        "      batch_size=BATCH_SIZE, \n",
        "      #classes = columns,\n",
        "      subset = 'validation'\n",
        "  )\n",
        "\n",
        "  return train_gen, val_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzon1sedRD-l",
        "colab_type": "text"
      },
      "source": [
        "# Cyclic LR\n",
        "\n",
        "It seems that our code gets stuck on a loss plateau. Let's try to implement the cyclic LR callback. \n",
        "\n",
        "The below callback implements a cyclical learning rate policy (CLR).\n",
        "The method cycles the learning rate between two boundaries with\n",
        "some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
        "The amplitude of the cycle can be scaled on a per-iteration or per-cycle basis.This class has three built-in policies, as put forth in the paper.\n",
        "    \"triangular\":\n",
        "        A basic triangular cycle w/ no amplitude scaling.\n",
        "    \"triangular2\":\n",
        "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    \"exp_range\":\n",
        "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
        "        cycle iteration.\n",
        "    For more detail, please see paper.\n",
        "    \n",
        "    # Example\n",
        "        ```python\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., mode='triangular')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "    \n",
        "    Class also supports custom scaling functions:\n",
        "        ```python\n",
        "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., scale_fn=clr_fn,\n",
        "                                scale_mode='cycle')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ``` \n",
        "        base_lr: initial learning rate which is the\n",
        "            lower boundary in the cycle.\n",
        "        max_lr: upper boundary in the cycle. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore \n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size: number of training iterations per\n",
        "            half cycle. Authors suggest setting step_size\n",
        "            2-8 x training iterations in epoch.\n",
        "        mode: one of {triangular, triangular2, exp_range}.\n",
        "            Default 'triangular'.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "        gamma: constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "        scale_fn: Custom scaling policy defined by a single\n",
        "            argument lambda function, where \n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            mode paramater is ignored \n",
        "        scale_mode: {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on \n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle). Default is 'cycle'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNygsqKPRCm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import *\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    \n",
        "    def __init__(self, base_lr=0.001, max_lr=0.01, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zPMTVNLQnVVZ"
      },
      "source": [
        "# The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZStKfeQanVVb",
        "colab": {}
      },
      "source": [
        "# Building on top of the base:\n",
        "from keras.applications import Xception\n",
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization, Conv2D, GlobalAveragePooling2D\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "\n",
        "def build_model():\n",
        "  # The convolutional base:\n",
        "  model_base = Xception(\n",
        "      weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "      )\n",
        "  #model_base.trainable = False\n",
        "  # Unfreezing all the layers:\n",
        "  for layer in model_base.layers:\n",
        "      layer.trainable = True\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(model_base) # Adding the base as a layer\n",
        "  model.add(GlobalAveragePooling2D())\n",
        "  model.add(Dense(1024, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "  #model.add(Flatten())\n",
        "  #model.add(Dense(1024, activation='relu'))\n",
        "  #model.add(Dropout(0.25))\n",
        "  model.add(Dense(5, activation='sigmoid'))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWokRCvVkZXx",
        "colab_type": "text"
      },
      "source": [
        "# Training and Validating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwJylaZwyDMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "dbd880bd-a347-4afc-a12a-da8d27dfd08c"
      },
      "source": [
        "from keras.metrics import AUC, categorical_accuracy as catacc\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "auc = AUC()\n",
        "adam = Adam(learning_rate=0.00005) # 1/10th of default\n",
        "\n",
        "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=2)\n",
        "mc = ModelCheckpoint(\n",
        "    filepath='xception-keras-2.h5', verbose=1 #, save_best_only=True\n",
        ")\n",
        "clr = CyclicLR(mode='exp_range')\n",
        "cb_list = [es, mc]\n",
        "\n",
        "model = build_model()\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=adam,\n",
        "    metrics=['acc', auc, catacc]\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_LRbchQjW0a",
        "colab_type": "code",
        "outputId": "663278de-1248-42d5-949a-cf2e9abf3400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train_gen, val_gen = get_gen()\n",
        "\n",
        "# Training constants:\n",
        "TRAIN_STEPS = train_gen.n//BATCH_SIZE\n",
        "VAL_STEPS   = val_gen.n//BATCH_SIZE\n",
        "N_EPOCHS = 5"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 201284 validated image filenames.\n",
            "Found 22364 validated image filenames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHPtOc4gzJoz",
        "colab_type": "code",
        "outputId": "5177face-242f-45ba-d12d-9a87490859fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        }
      },
      "source": [
        "history = model.fit_generator(\n",
        "    train_gen,\n",
        "    steps_per_epoch=TRAIN_STEPS,\n",
        "    epochs=N_EPOCHS,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=VAL_STEPS,\n",
        "    callbacks = cb_list\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "6290/6290 [==============================] - 4347s 691ms/step - loss: 0.4376 - acc: 0.7983 - auc_1: 0.8007 - categorical_accuracy: 0.3926 - val_loss: 0.3620 - val_acc: 0.8288 - val_auc_1: 0.8452 - val_categorical_accuracy: 0.4163\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.36202, saving model to xception-keras-2.h5\n",
            "Epoch 2/10\n",
            "6290/6290 [==============================] - 4297s 683ms/step - loss: 0.3922 - acc: 0.8195 - auc_1: 0.8456 - categorical_accuracy: 0.4332 - val_loss: 0.3662 - val_acc: 0.8347 - val_auc_1: 0.8546 - val_categorical_accuracy: 0.4617\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.36202\n",
            "Epoch 3/10\n",
            "6290/6290 [==============================] - 4280s 680ms/step - loss: 0.3842 - acc: 0.8236 - auc_1: 0.8532 - categorical_accuracy: 0.4373 - val_loss: 0.3652 - val_acc: 0.8346 - val_auc_1: 0.8554 - val_categorical_accuracy: 0.4322\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.36202\n",
            "Epoch 4/10\n",
            "6290/6290 [==============================] - 4313s 686ms/step - loss: 0.3778 - acc: 0.8270 - auc_1: 0.8589 - categorical_accuracy: 0.4383 - val_loss: 0.3712 - val_acc: 0.8359 - val_auc_1: 0.8586 - val_categorical_accuracy: 0.4812\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.36202\n",
            "Epoch 5/10\n",
            "6290/6290 [==============================] - 4373s 695ms/step - loss: 0.3718 - acc: 0.8301 - auc_1: 0.8641 - categorical_accuracy: 0.4473 - val_loss: 0.3393 - val_acc: 0.8365 - val_auc_1: 0.8590 - val_categorical_accuracy: 0.4470\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.36202 to 0.33926, saving model to xception-keras-2.h5\n",
            "Epoch 6/10\n",
            "6290/6290 [==============================] - 4328s 688ms/step - loss: 0.3652 - acc: 0.8331 - auc_1: 0.8697 - categorical_accuracy: 0.4497 - val_loss: 0.4815 - val_acc: 0.8350 - val_auc_1: 0.8557 - val_categorical_accuracy: 0.3558\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.33926\n",
            "Epoch 7/10\n",
            " 584/6290 [=>............................] - ETA: 1:01:26 - loss: 0.3563 - acc: 0.8385 - auc_1: 0.8770 - categorical_accuracy: 0.4448"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8bc85831ffde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVAL_STEPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3798\u001b[0m     return nest.pack_sequence_as(\n\u001b[1;32m   3799\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3800\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3801\u001b[0m         expand_composites=True)\n\u001b[1;32m   3802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3798\u001b[0m     return nest.pack_sequence_as(\n\u001b[1;32m   3799\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3800\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3801\u001b[0m         expand_composites=True)\n\u001b[1;32m   3802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYuegKKSh75h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNlSawxuGJPr",
        "colab_type": "text"
      },
      "source": [
        "#### Stuff to try out:\n",
        "- Vary the learning rate, reduce it, use vanilla Xception (unfreeze everything)\n",
        "\n",
        "- Play with the classifier architecture:\n",
        "  - GlobalAveragePooling\n",
        "  - Densely connected "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mertyO7Raf9K",
        "colab_type": "text"
      },
      "source": [
        "#### Classifier Architectures:\n",
        "\n",
        "- A1: AUC: (76, 71) | cat_acc: (32, 30)\n",
        "```\n",
        "model = Sequential()\n",
        "  model.add(model_base) # Adding the base as a layer\n",
        "  model.add(Conv2D(500, kernel_size = (3,3), activation='relu', input_shape = (IMAGE_SIZE,IMAGE_SIZE,3)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(GlobalAveragePooling2D())\n",
        "  model.add(Dense(1024, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(5, activation='sigmoid'))\n",
        "  ```\n",
        "    \n",
        "- A2: AUC: (76, 69) | cat_acc: (31, 42)\n",
        "```\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(5, activation='sigmoid'))\n",
        "```\n",
        "  \n",
        "- A3: AUC: (87, 86) | cat_acc: (46, 45)\n",
        "```\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(5, activation='sigmoid'))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgqqwQ9N9a6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}